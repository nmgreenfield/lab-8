---
title: "Lab 8"
subtitle: "Cheese Gromit!"
editor: source
---

```{r, include=F}
library(httr)
library(curl)
library(rvest)
library(tidyverse)
library(purrr)
```


> **Goal:** Scrape information from <https://www.cheese.com> to obtain a dataset
> of characteristics about different cheeses, and gain deeper insight into your
> coding process. ðŸª¤

**Part 1:** Locate and examine the `robots.txt` file for this website. Summarize
what you learn from it.

```{r}
#| label: robot.txt

url <- "https://www.cheese.com/robots.txt"

con <- curl(url)
readLines(con)
```

The `robots.txt` file tells us that anyone is allowed to scrape and there are no disallowed sections. There are also no restrictions on when/how often the website can be scraped. 


**Part 2:** Learn about the `html_attr()` function from `rvest`. Describe how
this function works with a small example.

```{r}
#| label: html_attr

cheese_page <- read_html("https://www.cheese.com")

#Using html_attr() to find the destinations of all links on the cheese.com home page
cheese_links <- cheese_page %>% 
  html_elements("a") %>% 
  html_attr("href")

head(cheese_links)

#Using html_attr() to find the sources of all images on the cheese.com home page
cheese_pics <- cheese_page %>%
  html_elements("img") %>%
  html_attr("src")

head(cheese_pics)

```

The `html_attr()` function takes an html document, node set, node, or session and retrieves a named attribute as defined by the user. The function will return a character vector, so post-processing may be needed to convert character strings into usable links or numbers. 

**Part 3:** (Do this alongside Part 4 below.) I 
used [ChatGPT](https://chat.openai.com/chat) to start the process of scraping
cheese information with the following prompt:

> Write R code using the rvest package that allows me to scrape cheese
> information from cheese.com.

Fully document your process of checking this code. Record any observations you
make about where ChatGPT is useful / not useful.

```{r}
#| eval: false
#| label: small-example-of-getting-cheese-info

# Load required libraries
library(rvest)
library(dplyr)

# Define the URL
url <- "https://www.cheese.com/alphabetical"

# Read the HTML content from the webpage
webpage <- read_html(url)

# Extract the cheese names and URLs
cheese_data <- webpage %>%
  
#NOTE - html_nodes is a deprecated function (better to use html_elements)
  
  html_nodes(".cheese-item") %>% 
  
#NOTE - nothing is returned when searching for a ".cheese-item" node
  
  html_nodes("a") %>%
  html_attr("href") %>%
  
#NOTE - since no cheese names were passed through the pipeline above, the only output is "cheese.com"
  
  paste0("https://cheese.com", .)

cheese_names <- webpage %>%
  html_nodes(".cheese-item h3") %>%
  html_text()

#NOTE - the code above returns another empty nodeset

# Create a data frame to store the results

#NOTE - there is no data to compile into a data frame, so this chunk throws an error

cheese_df <- data.frame(Name = cheese_names,
                        URL = cheese_data,
                        stringsAsFactors = FALSE)

# Print the data frame
print(cheese_df)
```

ChatGPT does provide some useful syntax for processing the cheese.com webpage. However, it uses outdated functions (html_nodes) and appears to hallucinate some of the node names. With some slight modifications, this generated code would be able to produce the desired results.

**Part 4:** Obtain the following information for **all** cheeses in the
database:

-   cheese name
-   URL for the cheese's webpage (e.g., <https://www.cheese.com/gouda/>)
-   whether or not the cheese has a picture (e.g., 
[gouda](https://www.cheese.com/gouda/) has a picture, but 
[bianco](https://www.cheese.com/bianco/) does not).

To be kind to the website owners, please add a 1 second pause between page
queries. (Note that you can view 100 cheeses at a time.)

```{r}
#| label: helper function

get_text_from_page <- function(page, css_selector) {
    
  page %>%
    html_elements(css_selector) %>%
    html_text(trim = T)
}

get_url_from_page <- function(page, css_selector) {
    
  page %>%
    html_elements(css_selector) %>%
    html_attr("href")
}

get_image_from_page <- function(page, css_selector) {
    
  page %>%
    html_elements(css_selector) %>%
    html_attr("src")
}

scrape_page <- function(url) {
    
    # 1 second crawl delay
    Sys.sleep(1)
    
    # Read the page
    page <- read_html(url)
    
    # Grab elements from the page
    cheese_names <- get_text_from_page(page, ".product-item")
    cheese_url <- get_url_from_page(page, ".product-item a")
    cheese_image <- get_image_from_page(page, ".product-item img")
    
    # Clean cheese names
    cheese_names <- cheese_names %>%
      trimws()
    
    cheese_names <- ifelse(
      str_detect(cheese_names, "Stores >"),
      str_trim(str_extract(cheese_names, "[^\n]+$")),
      cheese_names
      )
    
    # Full cheese URL
    base_url <- "https://www.cheese.com"
    cheese_url <- paste0(base_url, cheese_url)
    cheese_url <- unique(cheese_url[!grepl("store", cheese_url)]) #remove store links + duplicates
    
    # Find cheeses with image
    has_image <- ifelse(grepl("static", cheese_image), "No", "Yes")
    
    #Put page elements into a dataframe
    cheeses <- data.frame(
      cheese = cheese_names,
      url = cheese_url,
      image = has_image
    )
    
    return(cheeses)
}

```


```{r}
#| label: All cheese info

base_url <- "https://www.cheese.com/alphabetical/?per_page=100"

urls_all_pages <- c(str_c(base_url,
                          "&page=",
                          1:21)
                     )

all_pages <- map(urls_all_pages, scrape_page)

all_cheeses <- bind_rows(all_pages)

head(all_cheeses)
```


**Part 5:** When you go to a particular cheese's page (like 
[gouda](https://www.cheese.com/gouda/)), you'll see more detailed information
about the cheese. For [**just 10**]{.underline} of the cheeses in the database,
obtain the following detailed information:

-   milk information
-   country of origin
-   family
-   type
-   flavour

(Just 10 to avoid overtaxing the website! Continue adding a 1 second pause
between page queries.)

```{r}
#| label: helper functions 2

clean_info <- function(text, fallback) {
  if (length(text) == 0) {
    return(fallback)
  } else {
    cleaned <- sub(".*?:\\s*", "", text) # Extract everything after ": "
    return(trimws(cleaned))
  }
}

scrape_cheese <- function(url) {
    
    # 1 second crawl delay
    Sys.sleep(1)
    
    # Read the page
    page <- read_html(url)
    
    # Grab elements from the page
    milk <- get_text_from_page(page, ".summary_milk")
    country <- get_text_from_page(page, ".summary_country")
    family <- get_text_from_page(page, ".summary_family")
    type <- get_text_from_page(page, ".summary_moisture_and_type")
    flavor <- get_text_from_page(page, ".summary_taste")
    
    # Clean elements
    milk_clean <- clean_info(milk, "No milk information available")
    country_clean <- clean_info(country, "No country information available")
    family_clean <- clean_info(family, "No family information available")
    type_clean <- clean_info(type, "No type information available")
    flavor_clean <- clean_info(flavor, "No flavor information available")
    
    # Puts elements into data frame
    cheese <- data.frame(
      milk = milk_clean,
      country = country_clean,
      family = family_clean,
      type = type_clean,
      flavor = flavor_clean
    )
}
```


```{r}
#| label: cheese detail info

sampled_cheeses <- all_cheeses %>%
  sample_n(10)

detailed_cheeses <- sampled_cheeses$url %>%
  map_df(scrape_cheese)

final_cheese_info <- bind_cols(
  sampled_cheeses %>% select(cheese),
  detailed_cheeses
)

print(final_cheese_info)
```



**Part 6:** Evaluate the code that you wrote in terms of **efficiency**. To
what extent do your function(s) adhere to the **principles for writing good functions**?
To what extent are your **functions efficient**? To what extent is your 
**iteration of these functions efficient**? 