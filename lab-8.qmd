---
title: "Lab 8"
subtitle: "Cheese Gromit!"
editor: source
---

```{r, include=F}
library(httr)
library(curl)
library(rvest)
library(tidyverse)
```


> **Goal:** Scrape information from <https://www.cheese.com> to obtain a dataset
> of characteristics about different cheeses, and gain deeper insight into your
> coding process. ðŸª¤

**Part 1:** Locate and examine the `robots.txt` file for this website. Summarize
what you learn from it.

```{r}
#| label: robot.txt

url <- "https://www.cheese.com/robots.txt"

con <- curl(url)
readLines(con)
```

The `robots.txt` file tells us that anyone is allowed to scrape and there are no disallowed sections. There are also no restrictions on when/how often the website can be scraped. 


**Part 2:** Learn about the `html_attr()` function from `rvest`. Describe how
this function works with a small example.

```{r}
#| label: html_attr

cheese_page <- read_html("https://www.cheese.com")

#Using html_attr() to find the destinations of all links on the cheese.com home page
cheese_links <- cheese_page %>% 
  html_elements("a") %>% 
  html_attr("href")

head(cheese_links)

#Using html_attr() to find the sources of all images on the cheese.com home page
cheese_pics <- cheese_page %>%
  html_elements("img") %>%
  html_attr("src")

head(cheese_pics)

```

The `html_attr()` function takes an html document, node set, node, or session and retrieves a named attribute as defined by the user. The function will return a character vector, so post-processing may be needed to convert character strings into usable links or numbers. 

**Part 3:** (Do this alongside Part 4 below.) I 
used [ChatGPT](https://chat.openai.com/chat) to start the process of scraping
cheese information with the following prompt:

> Write R code using the rvest package that allows me to scrape cheese
> information from cheese.com.

Fully document your process of checking this code. Record any observations you
make about where ChatGPT is useful / not useful.

```{r}
#| eval: false
#| label: small-example-of-getting-cheese-info

# Load required libraries
library(rvest)
library(dplyr)

# Define the URL
url <- "https://www.cheese.com/alphabetical"

# Read the HTML content from the webpage
webpage <- read_html(url)

# Extract the cheese names and URLs
cheese_data <- webpage %>%
  
  #NOTE - html_nodes is a deprecated function (better to use html_elements)
  
  html_nodes(".cheese-item") %>% 
  
  #NOTE - nothing is returned when searching for a ".cheese-item" node
  
  html_nodes("a") %>%
  html_attr("href") %>%
  
  #NOTE - since no cheese names were passed through the pipeline above, the only output is "cheese.com"
  
  paste0("https://cheese.com", .)

cheese_names <- webpage %>%
  html_nodes(".cheese-item h3") %>%
  html_text()
 #NOTE - 

# Create a data frame to store the results
cheese_df <- data.frame(Name = cheese_names,
                        URL = cheese_data,
                        stringsAsFactors = FALSE)

# Print the data frame
print(cheese_df)
```

**Part 4:** Obtain the following information for **all** cheeses in the
database:

-   cheese name
-   URL for the cheese's webpage (e.g., <https://www.cheese.com/gouda/>)
-   whether or not the cheese has a picture (e.g., 
[gouda](https://www.cheese.com/gouda/) has a picture, but 
[bianco](https://www.cheese.com/bianco/) does not).

To be kind to the website owners, please add a 1 second pause between page
queries. (Note that you can view 100 cheeses at a time.)

**Part 5:** When you go to a particular cheese's page (like 
[gouda](https://www.cheese.com/gouda/)), you'll see more detailed information
about the cheese. For [**just 10**]{.underline} of the cheeses in the database,
obtain the following detailed information:

-   milk information
-   country of origin
-   family
-   type
-   flavour

(Just 10 to avoid overtaxing the website! Continue adding a 1 second pause
between page queries.)

**Part 6:** Evaluate the code that you wrote in terms of **efficiency**. To
what extent do your function(s) adhere to the **principles for writing good functions**?
To what extent are your **functions efficient**? To what extent is your 
**iteration of these functions efficient**? 